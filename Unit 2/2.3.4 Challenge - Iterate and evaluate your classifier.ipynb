{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 372\n",
      "Accuracy : 62.8%\n"
     ]
    }
   ],
   "source": [
    "# Grab and process the raw data.\n",
    "df = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", header=None)\n",
    "df.columns = ['review', 'type']\n",
    "\n",
    "#Set up the keywords\n",
    "\n",
    "keywords = [\"won\\'t\", 'refund', 'disappointed', 'horrible', 'not', 'hate', 'terrible', 'bad', \"don\\'t\", 'died', \n",
    "            'wasted', 'waste', 'return', 'useless', 'died', 'dead', 'broke', 'broken', 'excessive', \n",
    "            'problem', 'unusable', \"wouldn\\'t\", \"couldn\\'t\", 'garbage']\n",
    "\n",
    "#remove: worthless, misleading, unacceptable \n",
    "\n",
    "for key in keywords:\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "    df[str(key)] = df.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "    \n",
    "df['allcaps'] = df.review.str.isupper()\n",
    "\n",
    "#This switches it so negative reviews are True, positive reviews are False\n",
    "\n",
    "df['reviews'] = (df['type'] == 0)\n",
    "\n",
    "data = df[keywords + ['allcaps']]\n",
    "target = df['reviews']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb.fit(data, target)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb.predict(data)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data.shape[0],\n",
    "    (target != y_pred).sum()\n",
    "))\n",
    "\n",
    "print(\"Accuracy : {}%\".format(\n",
    "    bnb.score(data, target) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    500\n",
      "0    500\n",
      "Name: type, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df['type'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[478,  22],\n",
       "       [350, 150]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 350 of our 372 errors are from failing to identify a bad review (False negative - Type II Error)\n",
    " \n",
    " 22 of our 372 errors are from incorrectly identifying a good review (False positive - Type I Error)\n",
    " \n",
    " Sensitivity is the percentage of positives correctly identified, in our case 150/500 or 30%\n",
    " \n",
    " Specificity is the opposite and ours is  478/500 or 95.6%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.645\n",
      "Testing on Sample: 0.628\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.61, 0.65, 0.64, 0.64, 0.54, 0.66, 0.61, 0.65, 0.58])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 2 of Classifier (Attempt to Lower Overfit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 483\n",
      "Accuracy : 51.7%\n"
     ]
    }
   ],
   "source": [
    "# Grab and process the raw data.\n",
    "df2 = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", header=None)\n",
    "df2.columns = ['review', 'type']\n",
    "\n",
    "#Set up the keywords\n",
    "\n",
    "keywords2 = [\"hate\", \"refund\", \"useless\", \"stupid\", \"broken\", \"damaged\"]\n",
    "\n",
    "\n",
    "for key in keywords2:\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "    df2[str(key)] = df2.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "    \n",
    "df2['allcaps'] = df2.review.str.isupper()\n",
    "\n",
    "#This switches it so negative reviews are True, positive reviews are False\n",
    "\n",
    "df2['reviews'] = (df2['type'] == 0)\n",
    "\n",
    "data2 = df2[keywords2 + ['allcaps']]\n",
    "target2 = df2['reviews']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb2 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb2.fit(data2, target2)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb2.predict(data2)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data2.shape[0],\n",
    "    (target2 != y_pred).sum()\n",
    "))\n",
    "\n",
    "print(\"Accuracy : {}%\".format(\n",
    "    bnb2.score(data2, target2) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[495,   5],\n",
       "       [478,  22]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.645\n",
      "Testing on Sample: 0.628\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.61, 0.65, 0.64, 0.64, 0.54, 0.66, 0.61, 0.65, 0.58])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Analysis__: Worse output and no real change in the overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 3 of Classifier: Identifying Positives Instead of Negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 374\n",
      "Accuracy : 62.6%\n"
     ]
    }
   ],
   "source": [
    "# Grab and process the raw data.\n",
    "df3 = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", header=None)\n",
    "df3.columns = ['review', 'type']\n",
    "\n",
    "#Set up the keywords\n",
    "\n",
    "keywords3 = [\"love\", \"perfect\", \"exactly\", \"happy\", \"will\", \"repeat\", 'definitely', 'amazing', 'wonderful', \"10/10\",\n",
    "            \"10\", 'awesome', 'as expected', 'pleased', 'fulfills', 'recommended', 'impressed', 'works', \n",
    "             'reasonable', 'well', 'joy', 'great', 'good']\n",
    "\n",
    "\n",
    "for key in keywords3:\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "    df3[str(key)] = df3.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    "    )\n",
    "    \n",
    "\n",
    "# Positive Reviews are True, negative reviews are False\n",
    "\n",
    "df3['reviews'] = (df3['type'] == 1)\n",
    "\n",
    "data3 = df3[keywords3]\n",
    "target3 = df3['reviews']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb3 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb3.fit(data3, target3)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb3.predict(data3)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data3.shape[0],\n",
    "    (target3 != y_pred).sum()\n",
    "))\n",
    "\n",
    "print(\"Accuracy : {}%\".format(\n",
    "    bnb3.score(data3, target3) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[354, 146],\n",
       "       [480,  20]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 20% Holdout: 0.645\n",
      "Testing on Sample: 0.628\n"
     ]
    }
   ],
   "source": [
    "# Test your model with different holdout groups.\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Use train_test_split to create the necessary training and test groups\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.2, random_state=20)\n",
    "print('With 20% Holdout: ' + str(bnb.fit(X_train, y_train).score(X_test, y_test)))\n",
    "print('Testing on Sample: ' + str(bnb.fit(data, target).score(data, target)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64, 0.61, 0.65, 0.64, 0.64, 0.54, 0.66, 0.61, 0.65, 0.58])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(bnb, data, target, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No great diference between identifying by positive or negative review, so let's go back to the negatives and try to find some commonalities or look at unique words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Version 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of mislabeled points out of a total 1000 points : 385\n",
      "Accuracy : 61.5%\n"
     ]
    }
   ],
   "source": [
    "# Grab and process the raw data.\n",
    "df4 = pd.read_csv('amazon_cells_labelled.txt', sep=\"\\t\", header=None)\n",
    "df4.columns = ['review', 'type']\n",
    "\n",
    "#Set up the keywords\n",
    "\n",
    "keywords4 = ['doesnt', 'however', 'sucks', 'mistake', 'difficult', 'instructions', 'unreliable', 'later', \n",
    "             'none', 'week', 'nothing', 'player', 'horrible', 'cant', 'disappointment', 'worst', 'first' \n",
    "             'broke', '5', 'support', 'unfortunately', 'stay', 'disappointed', 'junk', 'company', 'terrible'\n",
    "             'poor', 'anything', 'completely', 'back', 'disappointing', 'old', 'went', 'talk', 'return', 'easily', \n",
    "             'useless', 'didnt', 'weak', 'buying', 'waste', 'unit', 'money', 'hate', 'crap', 'bad', 'customer']\n",
    "            \n",
    "\n",
    "for key in keywords4:\n",
    "    # Note that we add spaces around the key so that we're getting the word,\n",
    "    # not just pattern matching.\n",
    "    df4[str(key)] = df4.review.str.contains(\n",
    "        ' ' + str(key) + ' ',\n",
    "        case=False\n",
    ")\n",
    "    \n",
    "\n",
    "# Positive Reviews are True, negative reviews are False\n",
    "\n",
    "df4['reviews'] = (df4['type'] == 1)\n",
    "\n",
    "data4 = df4[keywords4]\n",
    "target4 = df4['reviews']\n",
    "\n",
    "# Our data is binary / boolean, so we're importing the Bernoulli classifier.\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Instantiate our model and store it in a new variable.\n",
    "bnb4 = BernoulliNB()\n",
    "\n",
    "# Fit our model to the data.\n",
    "bnb4.fit(data4, target4)\n",
    "\n",
    "# Classify, storing the result in a new variable.\n",
    "y_pred = bnb4.predict(data4)\n",
    "\n",
    "# Display our results.\n",
    "print(\"Number of mislabeled points out of a total {} points : {}\".format(\n",
    "    data4.shape[0],\n",
    "    (target4 != y_pred).sum()\n",
    "))\n",
    "\n",
    "print(\"Accuracy : {}%\".format(\n",
    "    bnb4.score(data4, target4) * 100\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[122, 378],\n",
       "       [  7, 493]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the confusion matrix\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target4, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we got a lot better at identifying negative reviews but a loottttt worse about accidentally identifying positive reviews as negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
